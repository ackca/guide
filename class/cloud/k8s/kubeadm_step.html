<!DOCTYPE html>
<html lang="zh-cn">
<head>
	<meta charset="UTF-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
	<link rel="icon" href="../../../img/k8s.svg" />
	<title>混合云-k8s-kubeadm安装</title>
	<script type="text/javascript" src="../../../js/head.js"></script>
	<script type="text/javascript">
		window.onload = createNavigation(kubeadm_setup);
	</script>
</head>
<body>

	<div class="container-fluid">
		<div id="divCommand">
			<script type="text/javascript">
				createInstruT0("前期准备（所有节点）", "i1");
					createInstruT1("系统组件配置", "i1-1");
						createInstruT2("关闭SELinux");
							let cmd = `
								setenforce 0
								sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("配置防火墙，放行相关流量");
							createInstruT3("Master节点");
								cmd = `
									firewall-cmd --permanent --add-port=6443/tcp       # API Server
									firewall-cmd --permanent --add-port=2379-2380/tcp  # etcd
									firewall-cmd --permanent --add-port=10250/tcp      # kubelet
									firewall-cmd --permanent --add-port=10257/tcp      # kube-controller-manager
									firewall-cmd --permanent --add-port=10259/tcp      # kube-scheduler
									firewall-cmd --permanent --add-port=179/tcp        # Calico(BGP)
									firewall-cmd --permanent --add-port=4789/udp       # VXLAN

									firewall-cmd --reload
								`;
								createLinuxTxtT3(cmd);

							createInstruT3("Worker节点");
								cmd = `
									firewall-cmd --permanent --add-port=10250/tcp        # kubelet
									firewall-cmd --permanent --add-port=30000-32767/tcp  # NodePort 服务
									firewall-cmd --permanent --add-port=179/tcp          # Calico(BGP)

									firewall-cmd --reload
								`;
								createLinuxTxtT3(cmd);

						createInstruT2("修改节点主机名为对应角色");
							cmd = `
								hostnamectl set-hostname Master1
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("网卡配置域名，实现可以直接通过主机名访问对应节点");
							cmd = `
								nmcli connection modify ens224 ipv4.dns-search itachi.tech

								nmcli connection reload ens224
								nmcli connection up ens224

								cat /etc/resolv.conf
								# 下面为输出
								# Generated by NetworkManager
								search itachi.tech
								nameserver 10.0.0.1
								# 输出结束
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("测试主机名解析");
							cmd = `
								nslookup master1.itachi.tech
								# 下面为输出
								Server:  10.0.0.1
								Address: 10.0.0.1#53

								Name: master1.itachi.tech
								Address: 10.0.0.11
								# 输出结束

								nslookup master1
								# 下面为输出
								Server: 10.0.0.1
								Address: 10.0.0.1#53

								Name: master1.itachi.tech
								Address: 10.0.0.11
								# 输出结束
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("关闭缓存");
							cmd = `
								swapoff -a

								sed -ri 's/.*swap.*/#&/' /etc/fstab
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("安装必要模块");
							cmd = `
								dnf install -y yum-utils iproute-tc wget git
								dnf update -y openssh-server
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("配置NTP");
							createInstruT3("一般默认安装");
								cmd = `
									dnf install -y chrony
								`;
								createLinuxTxtT3(cmd);

							createInstruT3("配置文件：/etc/chrony.conf，默认第3行，修改为阿里云ntp服务器");
								cmd = `
									sed -i '3c\\pool ntp.aliyun.com iburst' /etc/chrony.conf
								`;
								createLinuxTxtT3(cmd);
								createConfigImgT3("kubeadm_step/i1$1");

							createInstruT3("重启服务并查看状态");
								cmd = `
									systemctl restart chronyd
									systemctl enable chronyd

									systemctl status chronyd
								`;
								createLinuxTxtT3(cmd);
								createConfigImgT3("kubeadm_step/i1$2");

					createInstruT1("k8s组件配置", "i1-2");
						createInstruT2("加载必要模块");
							cmd = `
								modprobe overlay
								modprobe br_netfilter

								cat <<EOF | tee /etc/modules-load.d/containerd.conf
								overlay
								br_netfilter
								EOF

								cat <<EOF | tee /etc/sysctl.d/99-kubernetes-cri.conf
								net.bridge.bridge-nf-call-iptables = 1
								net.ipv4.ip_forward = 1
								net.bridge.bridge-nf-call-ip6tables = 1
								EOF

								sysctl --system
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("安装containerd（阿里云镜像源）");
							cmd = `
								dnf config-manager --add-repo=https://mirrors.aliyun.com/docker-ce/linux/rhel/docker-ce.repo
								dnf install -y containerd

								systemctl enable containerd
								systemctl start containerd
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("创建containerd配置文件");
							cmd = `
								containerd config default | tee /etc/containerd/config.toml
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("修改containerd配置，cgroup + sandbox image");
							cmd = `
								sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
								sed -i 's|sandbox_image = "registry.k8s.io/pause:3.8"|sandbox_image = "registry.aliyuncs.com/google_containers/pause:3.10"|g' /etc/containerd/config.toml
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("配置镜像加速器");
							createInstruAllTopT2("镜像加速器地址为hub.itachi.tech，多个加速器则配置多项");
							cmd = `
								sed -i '/\\[plugins."io.containerd.grpc.v1.cri".registry\\]/,/^$/s|config_path = ""|config_path = "/etc/containerd/certs.d"|' /etc/containerd/config.toml

								mkdir -p /etc/containerd/certs.d/docker.io

								cat > /etc/containerd/certs.d/docker.io/hosts.toml <<EOF
								server = "https://docker.io"

								[host."https://hub.itachi.tech"]
								  capabilities = ["pull", "resolve"]

								EOF

								mkdir -p /etc/containerd/certs.d/registry.k8s.io

								cat > /etc/containerd/certs.d/registry.k8s.io/hosts.toml <<EOF
								server = "https://registry.k8s.io"

								[host."https://hub.itachi.tech"]
								  capabilities = ["pull", "resolve"]

								EOF

								systemctl daemon-reload
								systemctl restart containerd
								systemctl enable containerd
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("测试拉取镜像");
							cmd = `
								ctr images pull docker.io/library/nginx:latest --hosts-dir=/etc/containerd/certs.d
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("安装kubelet kubeadm kubectl（阿里云镜像源）");
							cmd = `
								cat <<EOF | tee /etc/yum.repos.d/kubernetes.repo
								[kubernetes]
								name=Kubernetes
								baseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.31/rpm/
								enabled=1
								gpgcheck=1
								gpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.31/rpm/repodata/repomd.xml.key
								EOF

								dnf install -y kubelet-1.31.1 kubeadm-1.31.1  kubectl-1.31.1

								systemctl enable kubelet
								systemctl start kubelet
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("提前拉取所需镜像");
							cmd = `
								kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers
							`;
							createLinuxTxtT2(cmd);

				createInstruT0("部署k8s集群", "i2");
					createInstruT1("部署k8s集群", "i2-1");
						createInstruT2("初始化集群（任意一个master），注意替换命令中的相应参数");
							cmd = `
								<table class="cmdTableT3">
									<tbody>
										<tr>
											<td rowSpan="12" class="tdCliLogo" style="border-right-color: #E6DB74">
												<img src="../../../img/k8s.svg" class="imgBrand k8s"/>
											</td>
										</tr>
										<tr>
											<td colSpan="2">
												<span class="cmd"><span class="linuxCMD_cmd">kubectl&nbsp;&nbsp;init</span>&nbsp;&nbsp;\\
											</td>
										</tr>
										<tr>
											<td colSpan="2">
												<span class="cmd"><span class="linuxCMD_Para">&nbsp;&nbsp;&nbsp;&nbsp;--apiserver-advertise-address</span><span class="spanR">=</span><span class="variable spanYH">10.0.0.11</span>&nbsp;&nbsp;\\
											</td>
										</tr>
										<tr>
											<td colSpan="2">
												<span class="cmd"><span class="linuxCMD_Para">&nbsp;&nbsp;&nbsp;&nbsp;--image-repository</span>&nbsp;&nbsp;registry.aliyuncs.com/google_containers&nbsp;&nbsp;\\
											</td>
										</tr>
										<tr>
											<td colSpan="2">
												<span class="cmd"><span class="linuxCMD_Para">&nbsp;&nbsp;&nbsp;&nbsp;--pod-network-cidr</span><span class="spanR">=</span><span class="variable spanYH">192.169.0.0/16</span>&nbsp;&nbsp;\\
											</td>
										</tr>
										<tr>
											<td colSpan="2">
												<span class="cmd"><span class="linuxCMD_Para">&nbsp;&nbsp;&nbsp;&nbsp;--service-cidr</span><span class="spanR">=</span><span class="variable spanYH">172.16.0.0/16</span>&nbsp;&nbsp;\\
											</td>
										</tr>
										<tr>
											<td colSpan="2">
												<span class="cmd"><span class="linuxCMD_Para">&nbsp;&nbsp;&nbsp;&nbsp;--control-plane-endpoint</span><span class="spanR">=</span><span class="variable spanYH">master1</span>&nbsp;&nbsp;\\
											</td>
										</tr>
										<tr>
											<td colSpan="2">
												<span class="cmd"><span class="linuxCMD_Para">&nbsp;&nbsp;&nbsp;&nbsp;--upload-certs</span>
											</td>
										</tr>
										<tr class="cmdInstru">
											<td class="tdcmdInstru1"><span class="variable">10.0.0.11</span>：</td>
											<td class="tdcmdInstru2">该master的ip地址</td>
										</tr>
										<tr class="cmdInstru">
											<td class="tdcmdInstru1"><span class="variable">192.169.0.0/16</span>：</td>
											<td class="tdcmdInstru2">pod的子网，根据设计并不使用该子网，配置与实际使用不冲突的子网</td>
										</tr>
										<tr class="cmdInstru">
											<td class="tdcmdInstru1"><span class="variable">172.16.0.0/16</span>：</td>
											<td class="tdcmdInstru2">service的子网</td>
										</tr>
										<tr class="cmdInstru">
											<td class="tdcmdInstru1"><span class="variable">master1</span>：</td>
											<td class="tdcmdInstru2">该master的主机名</td>
										</tr>
									</tbody>
								</table>
							`
							document.writeln(cmd);

							cmd = `
								kubeadm init \\
								  --apiserver-advertise-address=10.0.0.11 \\
								  --image-repository registry.aliyuncs.com/google_containers \\
								  --pod-network-cidr=192.169.0.0/16 \\
								  --service-cidr=172.16.0.0/16 \\
								  --control-plane-endpoint=master1 \\
								  --upload-certs
							`;
							createLinuxTxtT2(cmd);

							createConfigImgT2("kubeadm_step/i2$1");

						createInstruT2("配置KUBECONFIG（所有master）");
							cmd = `
								mkdir -p $HOME/.kube
								cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
								chown $(id -u):$(id -g) $HOME/.kube/config

								export KUBECONFIG=/etc/kubernetes/admin.conf
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("为worker打上标签（任意一个master）");
							createInstruT3("所有节点添加完毕后查看状态");
								createInstruAllTopT3("NotReady为正常状态");
								cmd = `
									kubectl get nodes
									# 下面为输出
									NAME      STATUS     ROLES           AGE     VERSION
									master1   NotReady   control-plane   6m24s   v1.31.1
									master2   NotReady   control-plane   7m13s   v1.31.1
									master3   NotReady   control-plane   7m31s   v1.31.1
									worker1   NotReady   <none>          8m47s   v1.31.1
									worker2   NotReady   <none>          8m37s   v1.31.1
									worker3   NotReady   <none>          8m32s   v1.31.1
									# 输出结束
								`;
								createLinuxTxtT3(cmd);

							createInstruT3("所有计算节点打标签");
								let instruArr = new Array();
								instruArr[0] = createLinuxCmd("kubectl label node “worker1。 node-role.kubernetes.io/worker=worker");
								instruArr[1] = createCmdInstruSpan("“worker1。", "节点名");
								createCmdInstruT3(instruArr, "k8s");

								cmd = `
									kubectl label node worker1 node-role.kubernetes.io/worker=worker
									kubectl label node worker2 node-role.kubernetes.io/worker=worker
									kubectl label node worker3 node-role.kubernetes.io/worker=worker
								`;
								createLinuxTxtT3(cmd);

							createInstruT3("再次查看节点状态");
								cmd = `
									kubectl get nodes
									# 下面为输出
									NAME      STATUS     ROLES           AGE   VERSION
									master1   NotReady   control-plane   21m   v1.31.1
									master2   NotReady   control-plane   22m   v1.31.1
									master3   NotReady   control-plane   22m   v1.31.1
									worker1   NotReady   worker          24m   v1.31.1
									worker2   NotReady   worker          24m   v1.31.1
									worker3   NotReady   worker          24m   v1.31.1
									# 输出结束
								`;
								createLinuxTxtT3(cmd);

					createInstruT1("重新生成添加Master节点命令", "i2-2");
						cmd = `
							kubeadm token create --print-join-command --certificate-key $(kubeadm init phase upload-certs --upload-certs | tail -n 1)
						`;
						createLinuxTxtT1(cmd);

					createInstruT1("重新生成添加Worker节点命令", "i2-3");
						cmd = `
							kubeadm token create --print-join-command
						`;
						createLinuxTxtT1(cmd);

				createInstruT0("部署Calico网络（任意一master）", "i3");
					createInstruT1("官方资源", "i3-1");
						createInstruT2("官方文档地址：");
							createInstruAllTopT2("<a class='link' href='https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart' target='_blank'>https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart</a>");

						createInstruT2("原始文件位置：");
							createInstruAllTopT2("<a class='link' href='https://raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/tigera-operator.yaml' target='_blank'>https://raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/tigera-operator.yaml</a>");
							createInstruAllTopT2("<a class='link' href='https://raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/custom-resources.yaml' target='_blank'>https://raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/custom-resources.yaml</a>");

					createInstruT1("<br />初始化部署", "i3-2");
						createInstruT2("custom-resources.yaml文件内容如下：");
							createInstruAllTopT2("pod的子网默认为192.168.0.0/16，修改为与集群初始化时相同子网");
							createInstruAllTopT2("tigera-operator.yaml文件无需修改");
							cmd = `
								# This section includes base Calico installation configuration.
								# For more information, see: https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.Installation
								apiVersion: operator.tigera.io/v1
								kind: Installation
								metadata:
								  name: default
								spec:
								  # Configures Calico networking.
								  calicoNetwork:
								    ipPools:
								    - name: default-ipv4-ippool
								      blockSize: 26
								      cidr: 192.169.0.0/16  # 修改为与kubeadm init时相同的子网
								      encapsulation: VXLANCrossSubnet
								      natOutgoing: Enabled
								      nodeSelector: all()

								---

								# This section configures the Calico API server.
								# For more information, see: https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.APIServer
								apiVersion: operator.tigera.io/v1
								kind: APIServer
								metadata:
								  name: default
								spec: {}
							`;
							createYAMLT2(cmd);

						createInstruT2("安装Calico：间隔几秒分别部署两个文件，连续部署可能报错");
							cmd = `
								kubectl create -f tigera-operator.yaml
							`;
							createLinuxTxtT2(cmd);

							cmd = `
								kubectl create -f custom-resources.yaml
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("监控Calico创建资源");
							cmd = `
								watch kubectl get pods -n calico-system
								# 下面为输出，等待各个pod均为Ready，并为Running状态，得到类似如下状态
								Every 2.0s: kubectl get pods -n calico-system

								NAME                                      READY   STATUS    RESTARTS      AGE
								calico-kube-controllers-96c6dffb7-p6gz9   1/1     Running   0             4m12s
								calico-node-4phln                         1/1     Running   0             4m12s
								calico-node-92wwt                         1/1     Running   0             4m12s
								calico-node-p4655                         1/1     Running   0             4m12s
								calico-node-ptvb2                         1/1     Running   0             4m12s
								calico-node-ql664                         1/1     Running   0             4m12s
								calico-node-wfgct                         1/1     Running   0             4m12s
								calico-typha-8445b9945f-7pj87             1/1     Running   0             4m12s
								calico-typha-8445b9945f-g7h4c             1/1     Running   0             4m2s
								calico-typha-8445b9945f-l958p             1/1     Running   0             4m2s
								csi-node-driver-2z5f7                     2/2     Running   0             4m12s
								csi-node-driver-76qzq                     2/2     Running   0             4m12s
								csi-node-driver-cxrd7                     2/2     Running   0             4m12s
								csi-node-driver-d94tj                     2/2     Running   0             4m12s
								csi-node-driver-wz4jm                     2/2     Running   0             4m12s
								csi-node-driver-zbd5h                     2/2     Running   0             4m12s
								# 输出结束
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("去除管理节点的污点，使calico也能安装到管理节点上");
							cmd = `
								kubectl taint nodes --all node-role.kubernetes.io/control-plane-
								kubectl taint nodes --all node-role.kubernetes.io/master-
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("再次查看节点状态");
							cmd = `
								kubectl get nodes
								# 下面为输出
								NAME      STATUS   ROLES           AGE   VERSION
								master1   Ready    control-plane   31m   v1.31.1
								master2   Ready    control-plane   32m   v1.31.1
								master3   Ready    control-plane   32m   v1.31.1
								worker1   Ready    worker          34m   v1.31.1
								worker2   Ready    worker          34m   v1.31.1
								worker3   Ready    worker          34m   v1.31.1
								# 输出结束
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("安装calicoctl");
							cmd = `
								curl -O -L https://github.com/projectcalico/calico/releases/download/v3.29.1/calicoctl-linux-amd64
								chmod +x calicoctl-linux-amd64
								mv calicoctl-linux-amd64 /usr/local/bin/calicoctl
								export DATASTORE_TYPE=kubernetes
								export KUBECONFIG=/root/.kube/config
							`;
							createLinuxTxtT2(cmd);

					createInstruT1("为了管理方便，静态分配置各个节点的pod子网", "i3-3");
						createInstruT2("为各个节点创建其子网的标签");
							instruArr[0] = createLinuxCmd("kubectl label node “master1。 pod-cidr=“master1-cidr。 --overwrite");
							instruArr[1] = createCmdInstruSpan("“master1。", "节点名");
							instruArr[2] = createCmdInstruSpan("“master1-cidr。", "节点子网标签");
							createCmdInstruT2(instruArr, "k8s");
							cmd = `
								kubectl label node master1 pod-cidr=master1-cidr --overwrite
								kubectl label node master2 pod-cidr=master2-cidr --overwrite
								kubectl label node master3 pod-cidr=master3-cidr --overwrite
								kubectl label node worker1 pod-cidr=worker1-cidr --overwrite
								kubectl label node worker2 pod-cidr=worker2-cidr --overwrite
								kubectl label node worker3 pod-cidr=worker3-cidr --overwrite
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("创建节点的pod子网的配置文件");
							createInstruAllTopT2("根据实际修改每项的对应内容");
							cmd = `
								# cat static-cidr.yaml

								apiVersion: projectcalico.org/v3
								kind: IPPool
								metadata:
								  name: ippool-master1    # 地址池名，同时修改下面各项对应位置
								spec:
								  cidr: 192.168.11.0/24   # 子网，同时修改下面各项对应位置
								  blockSize: 24
								  vxlanMode: CrossSubnet
								  natOutgoing: false
								  nodeSelector: "pod-cidr == 'master1-cidr'"   # 使用该子网的节点配置的标签，同时修改下面各项对应位置

								---
								apiVersion: projectcalico.org/v3
								kind: IPPool
								metadata:
								  name: ippool-master2
								spec:
								  cidr: 192.168.12.0/24
								  blockSize: 24
								  vxlanMode: CrossSubnet
								  natOutgoing: false
								  nodeSelector: "pod-cidr == 'master2-cidr'"

								---
								apiVersion: projectcalico.org/v3
								kind: IPPool
								metadata:
								  name: ippool-master3
								spec:
								  cidr: 192.168.13.0/24
								  blockSize: 24
								  vxlanMode: CrossSubnet
								  natOutgoing: false
								  nodeSelector: "pod-cidr == 'master3-cidr'"

								---
								apiVersion: projectcalico.org/v3
								kind: IPPool
								metadata:
								  name: ippool-worker1
								spec:
								  cidr: 192.168.101.0/24
								  blockSize: 24
								  vxlanMode: CrossSubnet
								  natOutgoing: false
								  nodeSelector: "pod-cidr == 'worker1-cidr'"

								---
								apiVersion: projectcalico.org/v3
								kind: IPPool
								metadata:
								  name: ippool-worker2
								spec:
								  cidr: 192.168.102.0/24
								  blockSize: 24
								  vxlanMode: CrossSubnet
								  natOutgoing: false
								  nodeSelector: "pod-cidr == 'worker2-cidr'"

								---
								apiVersion: projectcalico.org/v3
								kind: IPPool
								metadata:
								  name: ippool-worker3
								spec:
								  cidr: 192.168.103.0/24
								  blockSize: 24
								  vxlanMode: CrossSubnet
								  natOutgoing: false
								  nodeSelector: "pod-cidr == 'worker3-cidr'"

							`;
							createYAMLT2(cmd);

						createInstruT2("应用配置文件");
							cmd = `
								kubectl apply -f static-cidr.yaml
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("查看ippool");
							cmd = `
								kubectl get ippool -o wide
								# 下面为输出
								NAME                  CREATED AT
								default-ipv4-ippool   2025-03-14T06:53:00Z
								ippool-master1        2025-03-14T06:57:47Z
								ippool-master2        2025-03-14T06:57:47Z
								ippool-master3        2025-03-14T06:57:47Z
								ippool-worker1        2025-03-14T06:57:47Z
								ippool-worker2        2025-03-14T06:57:47Z
								ippool-worker3        2025-03-14T06:57:47Z
								# 输出结束
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("删除默认的地址池");
							cmd = `
								calicoctl patch ippool default-ipv4-ippool -p '{"spec": {"disabled": true}}'
								calicoctl delete ippool default-ipv4-ippool
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("重启所有节点");
							createInstruAllTopT2("当前pod正在使用默认地址池，无法直接替换，重启后使用新创建的地址池");
							cmd = `
								reboot
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("查看所有pod的IP地址");
							createInstruAllTopT2("默认地址池default-ipv4-ippool，会生成一个新的，没有被真正删除，");
							createInstruAllTopT2("但各个pod不会再使用其192.169.0.0/16子网");
							cmd = `
								kubectl get pod -A -o wide
								# 下面为输出
								NAMESPACE          NAME                                       READY   STATUS    RESTARTS      AGE     IP               NODE      NOMINATED NODE   READINESS GATES
								calico-apiserver   calico-apiserver-6f489f7457-hmlbr          1/1     Running   4 (60m ago)   79m     192.168.103.4    worker3   <none>           <none>
								calico-apiserver   calico-apiserver-6f489f7457-kcb2q          1/1     Running   3 (60m ago)   79m     192.168.101.7    worker1   <none>           <none>
								calico-system      calico-kube-controllers-7f6f7685bb-hfmgq   1/1     Running   3 (60m ago)   79m     192.168.101.6    worker1   <none>           <none>
								calico-system      calico-node-2rnsc                          1/1     Running   3 (60m ago)   79m     10.0.0.11        master1   <none>           <none>
								calico-system      calico-node-cz8ts                          1/1     Running   3 (60m ago)   79m     10.0.0.101       worker1   <none>           <none>
								calico-system      calico-node-dz2zp                          1/1     Running   3 (60m ago)   79m     10.0.0.102       worker2   <none>           <none>
								calico-system      calico-node-xsr8t                          1/1     Running   3 (60m ago)   79m     10.0.0.103       worker3   <none>           <none>
								calico-system      calico-typha-75c6b49bcb-2cxl4              1/1     Running   3 (60m ago)   79m     10.0.0.102       worker2   <none>           <none>
								calico-system      calico-typha-75c6b49bcb-726tk              1/1     Running   3 (60m ago)   78m     10.0.0.103       worker3   <none>           <none>
								calico-system      csi-node-driver-k5nzl                      2/2     Running   6 (60m ago)   79m     192.168.101.8    worker1   <none>           <none>
								calico-system      csi-node-driver-ld7sd                      2/2     Running   6 (60m ago)   79m     192.168.11.4     master1   <none>           <none>
								calico-system      csi-node-driver-t2cm2                      2/2     Running   6 (60m ago)   79m     192.168.103.3    worker3   <none>           <none>
								calico-system      csi-node-driver-wscdp                      2/2     Running   6 (60m ago)   79m     192.168.102.2    worker2   <none>           <none>
								kube-system        coredns-855c4dd65d-h2tc6                   1/1     Running   3 (60m ago)   93m     192.168.101.5    worker1   <none>           <none>
								kube-system        coredns-855c4dd65d-qnbhd                   1/1     Running   3 (60m ago)   93m     192.168.11.3     master1   <none>           <none>
								kube-system        etcd-master1                               1/1     Running   6 (60m ago)   4h35m   10.0.0.11        master1   <none>           <none>
								kube-system        kube-apiserver-master1                     1/1     Running   6 (60m ago)   4h35m   10.0.0.11        master1   <none>           <none>
								kube-system        kube-controller-manager-master1            1/1     Running   6 (60m ago)   4h35m   10.0.0.11        master1   <none>           <none>
								kube-system        kube-proxy-7xd7b                           1/1     Running   6 (60m ago)   4h35m   10.0.0.101       worker1   <none>           <none>
								kube-system        kube-proxy-nzz9t                           1/1     Running   6 (60m ago)   4h35m   10.0.0.102       worker2   <none>           <none>
								kube-system        kube-proxy-qjdgt                           1/1     Running   6 (60m ago)   4h35m   10.0.0.103       worker3   <none>           <none>
								kube-system        kube-proxy-sf47h                           1/1     Running   6 (60m ago)   4h35m   10.0.0.11        master1   <none>           <none>
								kube-system        kube-scheduler-master1                     1/1     Running   6 (60m ago)   4h35m   10.0.0.11        master1   <none>           <none>
								tigera-operator    tigera-operator-76c4976dd7-xk2hd           1/1     Running   3 (60m ago)   79m     10.0.0.103       worker3   <none>           <none>
								# 输出结束
							`;
							createLinuxTxtT2(cmd);

					createInstruT1("测试网络", "i3-4");
						createInstruT2("Deployment");
							cmd = `
								# cat itachi-pod-dp.yaml

								apiVersion: apps/v1
								kind: Deployment
								metadata:
								  name: itachi-pod-dp # deployment的名字
								spec:
								  replicas: 3 # deployment必须指定副本数量
								  selector:
								    matchLabels:
								      app: itachi-pod-dp-label # 选择器匹配的标签，必须与下面的标签相同
								  template:
								    metadata:
								      labels:
								        app: itachi-pod-dp-label # 标签
								    spec:
								      containers:
								      - name: container-1-itachi # 容器的名字, 一个pod内可以有多个容器
								        image: nginx  # 容器的镜像
								        ports:
								        - containerPort: 80 # 只是一个信息, 类似于Dockerfile的EXPOSE
							`;
							createYAMLT2(cmd);

						createInstruT2("Deployment对应的Service");
							cmd = `
								# cat itachi-service-dp.yaml

								apiVersion: v1
								kind: Service
								metadata:
								  name: itachi-service-dp # service的名字
								spec:
								  selector:
								    app: itachi-pod-dp-label # service选择的标签，对应dp或ds的标签
								  ports:
								  - name: http
								    port: 8080      # 对外暴露的端口
								    targetPort: 80  # Pod的端口
								    protocol: TCP
							`;
							createYAMLT2(cmd);

						createInstruT2("应用Deployment其及Service");
							cmd = `
								kubectl apply -f itachi-pod-dp.yaml
								kubectl apply -f itachi-service-dp.yaml
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("查看对应信息");
							cmd = `
								kubectl get deployment
								# 下面为输出
								NAME            READY   UP-TO-DATE   AVAILABLE   AGE
								itachi-pod-dp   3/3     3            3           36s
								# 输出结束

								kubectl get pod
								# 下面为输出
								NAME                            READY   STATUS    RESTARTS   AGE
								itachi-pod-dp-c6f6c4d96-56h8b   1/1     Running   0          46s
								itachi-pod-dp-c6f6c4d96-8sgjr   1/1     Running   0          46s
								itachi-pod-dp-c6f6c4d96-gffg5   1/1     Running   0          46s
								# 输出结束

								kubectl get pod -o wide
								# 下面为输出
								NAME                            READY   STATUS    RESTARTS   AGE   IP               NODE      NOMINATED NODE   READINESS GATES
								itachi-pod-dp-c6f6c4d96-56h8b   1/1     Running   0          66s   192.168.101.10   worker1   <none>           <none>
								itachi-pod-dp-c6f6c4d96-8sgjr   1/1     Running   0          66s   192.168.103.6    worker3   <none>           <none>
								itachi-pod-dp-c6f6c4d96-gffg5   1/1     Running   0          66s   192.168.102.4    worker2   <none>           <none>
								# 输出结束

								kubectl get svc
								# 下面为输出
								NAME                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
								itachi-service-dp   ClusterIP   172.16.10.239   <none>        8080/TCP   4m1s
								kubernetes          ClusterIP   172.16.0.1      <none>        443/TCP    4h7m
								# 输出结束
							`;
							createLinuxTxtT2(cmd);

						createInstruT2("进入1个pod测试其它pod的nginx及service");
							instruArr = [];
							instruArr[0] = createLinuxCmd("kubectl exec -it “itachi-pod-dp-c6f6c4d96-56h8b。 -- bash");
							instruArr[1] = createCmdInstruSpan("“itachi-pod-dp-c6f6c4d96-56h8b。", "pod名");
							createCmdInstruT2(instruArr, "k8s");

							cmd = `
								kubectl exec -it itachi-pod-dp-c6f6c4d96-56h8b -- bash

								# 分别测试每个pod的nginx，通过curl 其ip地址测试
								# 测试第1个pod的ip地址
								root@itachi-pod-dp-c6f6c4d96-56h8b:/# curl 192.168.101.10
								# 下面为输出，nginx首页内容
								<!DOCTYPE html>
								<html>
								<head>
								<title>Welcome to nginx!</title>
								<style>
								html { color-scheme: light dark; }
								body { width: 35em; margin: 0 auto;
								font-family: Tahoma, Verdana, Arial, sans-serif; }
								</style>
								</head>
								<body>
								<h1>Welcome to nginx!</h1>
								<p>If you see this page, the nginx web server is successfully installed and
								working. Further configuration is required.</p>

								<p>For online documentation and support please refer to
								<a href="http://nginx.org/">nginx.org</a>.<br/>
								Commercial support is available at
								<a href="http://nginx.com/">nginx.com</a>.</p>

								<p><em>Thank you for using nginx.</em></p>
								</body>
								</html>
								# 输出结束

								# 测试第2个pod的ip地址，省略nginx输出内容
								root@itachi-pod-dp-c6f6c4d96-56h8b:/# curl 192.168.103.6

								# 测试第3个pod的ip地址，省略nginx输出内容
								root@itachi-pod-dp-c6f6c4d96-56h8b:/# curl 192.168.102.4

								# 测试service的ip地址，省略nginx输出内容
								root@itachi-pod-dp-c6f6c4d96-56h8b:/# curl 172.16.10.239:8080
							`;
							createLinuxTxtT2(cmd);

			</script>


		</div>
	</div>
</body>
</html>